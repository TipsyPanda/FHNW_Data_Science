---
title: "Project Part 2"
output: 
  html_notebook: 
    theme: cerulean
    highlight: textmate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

***

This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.

***

# Data Exploration & Preparation 
* Our goal in the second part of the assignment is to predict how good a (new) customer will pay 
back their credit card depts. In the data set application data from current customers (the first 18 
attributes) together with their status (last attribute; target) are given.  
* The attributes from the applications are 

Attribute Name | Explanation | Remarks
------------- | ------------- | -------------
ID | Client | number 
CODE_GENDER | Gender | 
FLAG_OWN_CAR | Is there a car | 
FLAG_OWN_REALTY | Is there a property | 
CNT_CHILDREN | Number of children | 
AMT_INCOME_TOTAL | Annual income | 
NAME_INCOME_TYPE | Income category | 
NAME_EDUCATION_TYPE | Education level | 
NAME_FAMILY_STATUS | Marital status | 
NAME_HOUSING_TYPE | Way of living | 
DAYS_BIRTH | Birthday | Count backwards from current day (0), -1 means yesterday 
DAYS_EMPLOYED | Start date of employment | Count backwards from current day(0). If positive, it means the person unemployed. 
FLAG_MOBIL | Is there a mobile phone | 
FLAG_WORK_PHONE | Is there a work phone | 
FLAG_PHONE | Is there a phone | 
FLAG_EMAIL | Is there an email | 
OCCUPATION_TYPE | Occupation | 
CNT_FAM_MEMBERS | Family size | 

* The last attribute status contains the “pay-back behavior”, i.e. when did that customer pay back 
their depts: 
  + 0: 1-29 days past due 
  + 1: 30-59 days past due 
  + 2: 60-89 days overdue 
  + 3: 90-119 days overdue 
  + 4: 120-149 days overdue 
  + 5: Overdue or bad debts, write-offs for more than 150 days 
  + C: paid off that month 
  + X: No loan for the month 
Please note: We are learning only the pay-back behavior. The decision, i.e. if we accept a customer or 
not, is done in another process step – not here!  


***

# Main task 
* Design your network. Why did you use a feed-forward network, or a convolutional or recursive 
network – and why not?  
* Use k-fold validation (with k = 10) to find the best hyperparameters for your network. 
* Use the average of the accuracy to evaluate the performance of your trained network. 
* Find a “reasonable” good model. Argue why that model is reasonable. If you are not able to find a 
reasonable good model, explain what you all did to find a good model and argue why you think 
that’s not a good model.  
* Save your trained neural network with save_model_hdf5. Also save your data sets you used 
for training, testing and validation. 

***

# Some hints 
* Data preprocessing is easier here; no feature engineering is needed. 
* You may be able to reuse parts of the exercises we used in our examples during lectures. 
* All in- and output values need to be floating numbers (or integers in exceptions) in the range of 
[0,1]. 
* Please note that a neural network expects a R matrix or vector, not data frames. Transform your 
data (e.g. a data frame) into a matrix with data.matrix if needed.  
* There are some models which show an accuracy higher than 90% (!) for training (and test) data – 
after learning more than 1000 epochs. 

***

# Important notes
* Single-label, Multiclass classification problem on page 73 in [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r)
* Spaces must be removed in between '```{r}' and '```', else an error with '<!-- rnb-source-end -->' will be produced
* K-Fold Validation on page 83ff and 94ff in [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r)
* Page 110, use Last-Layer activation softmax and loss function categorical_crossentropy

***

## Data import
```{r}
library(here)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tensorflow)
library(tfdatasets)
library(keras)
library(caret)
#LOAD DATA
setwd(getwd())
dataIn = "../Data/Dataset-part-2.csv"
data_in <- read.csv(dataIn,header = TRUE, sep =',')
View(data_in)
data <- data.frame(data_in)
summary(data)
```
##Cleanup
```{r}
data <- data %>% select(-ID)
cols <- c("CODE_GENDER","FLAG_OWN_CAR","FLAG_OWN_REALTY","NAME_INCOME_TYPE","NAME_EDUCATION_TYPE", "NAME_FAMILY_STATUS", "NAME_HOUSING_TYPE","FLAG_MOBIL","FLAG_WORK_PHONE","FLAG_PHONE","FLAG_EMAIL", "OCCUPATION_TYPE","status")
cols
data[cols] <- lapply(data[cols],factor)

summary(data)

# One-Hot-Encoding for CODE_GENDER
dmy <- dummyVars(" ~ CODE_GENDER", data = data)
trsf <- data.frame(predict(dmy, newdata = data)) 
data <- cbind(data, trsf)
data <- data %>% select(-CODE_GENDER)

# One-Hot-Encoding for FLAG_OWN_CAR 
dmyCar <- dummyVars(" ~ FLAG_OWN_CAR", data = data)
trsfCar <- data.frame(predict(dmyCar, newdata = data))
data <- cbind(data, trsfCar)
data <- data %>% select(-FLAG_OWN_CAR)

# One-Hot-Encoding for FLAG_OWN_REALTY   
dmyRealty <- dummyVars(" ~ FLAG_OWN_REALTY", data = data)
trsfRealty <- data.frame(predict(dmyRealty, newdata = data))
data <- cbind(data, trsfRealty)
data <- data %>% select(-FLAG_OWN_REALTY)

# One-Hot-Encoding for NAME_INCOME_TYPE   
dmyNIT <- dummyVars(" ~ NAME_INCOME_TYPE", data = data)
trsfNIT <- data.frame(predict(dmyNIT, newdata = data))
data <- cbind(data, trsfNIT)
data <- data %>% select(-NAME_INCOME_TYPE)

# One-Hot-Encoding for NAME_EDUCATION_TYPE   
dmyNET <- dummyVars(" ~ NAME_EDUCATION_TYPE", data = data)
trsfNET <- data.frame(predict(dmyNET, newdata = data))
data <- cbind(data, trsfNET)
data <- data %>% select(-NAME_EDUCATION_TYPE)

# One-Hot-Encoding for NAME_FAMILY_STATUS   
dmyNFS <- dummyVars(" ~ NAME_FAMILY_STATUS", data = data)
trsfNFS <- data.frame(predict(dmyNFS, newdata = data))
data <- cbind(data, trsfNFS)
data <- data %>% select(-NAME_FAMILY_STATUS)

# One-Hot-Encoding for NAME_HOUSING_TYPE   
dmyNHT <- dummyVars(" ~ NAME_HOUSING_TYPE", data = data)
trsfNHT <- data.frame(predict(dmyNHT, newdata = data))
data <- cbind(data, trsfNHT)
data <- data %>% select(-NAME_HOUSING_TYPE)

# Remove FLAG_MOBIL, it is always 1
data <- data %>% select(-FLAG_MOBIL)

# One-Hot-Encoding for FLAG_WORK_PHONE  
# Not needed, already 1 or 0
# dmyFWP <- dummyVars(" ~ FLAG_WORK_PHONE", data = data)
# trsfFWP <- data.frame(predict(dmyFWP, newdata = data))
# data <- cbind(data, trsfFWP)
# data <- data %>% select(-FLAG_WORK_PHONE)

# One-Hot-Encoding for FLAG_PHONE  
# Not needed, already 1 or 0
# dmyFP <- dummyVars(" ~ FLAG_PHONE", data = data)
# trsfFP <- data.frame(predict(dmyFP, newdata = data))
# data <- cbind(data, trsfFP)
# data <- data %>% select(-FLAG_PHONE)

# One-Hot-Encoding for FLAG_EMAIL 
# Not needed, already 1 or 0
# dmyFE <- dummyVars(" ~ FLAG_EMAIL", data = data)
# trsfFE <- data.frame(predict(dmyFE, newdata = data))
# data <- cbind(data, trsfFE)
# data <- data %>% select(-FLAG_EMAIL)

# One-Hot-Encoding for OCCUPATION_TYPE     
dmyOT <- dummyVars(" ~ OCCUPATION_TYPE ", data = data)
trsfOT <- data.frame(predict(dmyOT, newdata = data))
data <- cbind(data, trsfOT)
data <- data %>% select(-OCCUPATION_TYPE )

summary(data)

# Normalizing data
preprocessParams <- preProcess(data$CNT_CHILDREN, method=c("center", "scale"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, iris[,1:4])

mean <- apply(data, 2, mean)
std <- apply(data, 2, sd)
dataScaled <- scale(data$CNT_CHILDREN, center = mean, scale = std)

summary(dataScaled)
```
##K-Fold-Validation
```{r}
   # 75% of the sample size
  smp_size <- floor(0.75 * nrow(data))
  
  # set the seed to make your partition reproducible
  set.seed(123)
  data_ind <- sample(seq_len(nrow(data)), size = smp_size)
  train <- as_tibble(data[data_ind, ])
  test <- as_tibble(data[-data_ind, ])

  train_no_status <- select(train,-c(status))
  train_status <- select(train,c(status))
  
  
#define model
build_model <- function() {
  model <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", 
                input_shape = dim(train)[[2]]) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1) 
    
  model %>% compile(
    optimizer = "rmsprop", 
    loss = "mse", 
    metrics = c("mae")
  )
}

  
k <- 10
indices <- sample(1:nrow(train))
folds <- cut(indices, breaks = k, labels = FALSE)

num_epochs <- 100
all_scores <- c()
  
 for (i in 1:k) {
    
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE) 
  val_data <- train_no_status[val_indices,]
  val_targets <- train_status[val_indices,]
   
   # Prepare the training data: data from all other partitions
  partial_train_data <- train_no_status[-val_indices,]
  partial_train_targets <- train_status[-val_indices,]
  
  #train & eval model
  #model <- build_model()
  
  
  #model %>% fit(partial_train_data, partial_train_targets,
  #              epochs = num_epochs, batch_size = 1, verbose = 0)
  
  #validate
  #results <- model %>% evaluate(val_data, val_targets, verbose = 0)
  
  #all_scores <- c(all_scores, results["mae"])
 }


  
```

```{r}
all_scores
mean(all_scores)
```


```{r}
  ## Take Model1 and Scale data
  ##scale and preprocess training and test data
  #library(caret)
  pre_proc_val <- preProcess(train, method = c("center", "scale"))  
  train = predict(pre_proc_val, train)
  test = predict(pre_proc_val, test)
  train <- tensor_slices_dataset(train)
  test <- tensor_slices_dataset(test)
  summary(train)
  
```


```{r, results='hide'}
library(keras)
#https://www.tensorflow.org/tutorials/structured_data/feature_columns

#center and scale --> min max
#one hot encoding
# Matrix and vector

# model = tf.keras.Sequential([
#   feature_layer,
#   layers.Dense(128, activation='relu'),
#   layers.Dense(128, activation='relu'),
#   layers.Dropout(.1),
#   layers.Dense(1)
# ])
# 
# model.compile(optimizer='adam',
#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#               metrics=['accuracy'])
# 
# model.fit(train_ds,
#           validation_data=val_ds,
#           epochs=10)

```



```{r}
str(train_data[[1]])
```