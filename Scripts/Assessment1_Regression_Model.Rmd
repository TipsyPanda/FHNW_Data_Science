---
title: "Data Science - Regression Task"
author: "Group1"
date: "2022-12-09"
output:
  pdf_document: 
    toc: true
    toc_depth: 3
  html_document: 
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'png')
```

\pagebreak

#Abstract

In this study, pre-cleaned data was further processed for training using one-hot encoding, removal of near zero variance variables, and scaling between 0 and 1. Initially, linear regression, random forest, and gradient boosting models were applied. For each model, several commonly used hyperparameters were utilized to obtain a preliminary assessment of performance. After determining that XGboost achieved the best results, the XGboost approach was selected for further optimization through hyperparameter tuning. The optimized XGboost model was trained on the training set using the optimal set of hyperparameters and evaluated using standard metrics (MSE, RMSE, MAE, RSQ). The fit of the model was also assessed through visualization.

```{r Load libraries and data, include=FALSE}
library(tidyverse)
library(tidymodels)
library(themis)
library(randomForest)
library(ranger)
library(here)
library(xgboost)
library(gridExtra)
library(vip) #variable important plots
options(java.parameters = "-Xmx16g")
getwd()
lcdata <- readRDS("../Data/Out/cleanData.rds")
lcdata <- data.frame(lcdata %>% dplyr::select(int_rate,loan_amnt, term,installment,home_ownership, verification_status,purpose, annual_inc_merged, dti_merged, revol_util,revol_bal)) #region,mths_since_last_major_derog_cat,
```

\pagebreak
## General preparation of the dataset for machine learning

As a first step the class attribute "int_rate" is analyzed in a boxplot:

```{r echo=FALSE}
#Check Boxplot and histogram
boxplot(lcdata$int_rate, main ="Boxplot with outliers")
```

It is observed that there are several outliers at the upper end of the data, which can be identified in the boxplot through the $out column. These outliers can be removed by eliminating the $out column from the boxplot. The analysis also reveals that the class attribute is skewed to the right, which will be addressed through stratified sampling in a later stage of the assesment

```{r}
#remove outliers
outliers  <- boxplot(lcdata$int_rate, plot=FALSE)$out
lcdata<- lcdata[-which(lcdata$int_rate %in% outliers),]

cols <- c( "home_ownership", "purpose")
lcdata[cols] <- lapply(lcdata[cols], factor)
```


\pagebreak
Summary of the data set after initial cleaning steps:

```{r Summary of dataset, echo=FALSE, warning=FALSE}
hist1 <- ggplot(data.frame(int_rate=lcdata$int_rate), aes(x=int_rate)) + geom_histogram(binwidth = 30) 
boxp1 <- ggplot(data.frame(int_rate=lcdata$int_rate), aes(y=int_rate)) + geom_boxplot() 
grid.arrange(hist1,boxp1, ncol = 2, top = "Distribution of the class variable int_rate")
```

## Create a training and a test set

We use the initial_split(), training() and testing() functions to split our data into a training set and a test set using a 80/20 distribution. This is done so we can directly validate our trained model on the test data.

```{r Partition data into training and test sets}
set.seed(713)
trainIndex <- initial_split(lcdata, prop = 0.8, strata = int_rate) #stratify the class
#attribute since it is right skewed.This bins the data into quartiles
trainingSet <- training(trainIndex)
testSet <- testing(trainIndex)
```

## Pre-process the data to be used with the models

Since many predictive models perform better with numeric features, one-hot encoding is applied to the factor variables to transform them into a format where each level has its own column (using the step_dummy() function). Near zero variance predictors are also removed (using the step_nzv() function, although no predictors are affected in this case) and the predictors are normalized (using the step_range() function) to fall between 0 and 1, as the average waiting time cannot be negative. This process helps to improve the performance of the predictive models.

```{r Create a recipe for preproc}
set.seed(12)
preprocRecipe <-
  recipe(int_rate ~., data = trainingSet) %>%
  step_dummy(all_nominal()) %>%
  step_nzv(all_predictors(), -all_nominal()) %>%
  step_range(all_predictors(), -all_nominal(), min = 0, max = 1)
```

In this step the above defined receipt is extracted using the `prep()` function, and then use the `bake()` function to transform a set of data based on that recipe.

```{r Prep and bake the defined recipe}
trainingSet_processed <- preprocRecipe %>%
  prep(trainingSet) %>%
  bake(trainingSet)
testSet_processed <- preprocRecipe %>%
  prep(testSet) %>%
  bake(testSet)

write.csv(trainingSet_processed, "../Data/Out/trainingset.csv")
write.csv(testSet_processed, "../Data/Out/testset.csv")

```

We have now used the recipe steps to create fully processed training and test sets. We are ready to train machine learning algorithms.

## Create different regression models to compare

The class variable "int_rate" is numeric and therefore considered a regression task. To get an idea which type of model is worth further investigation three models were chosen for a first iteration:

-   multiple linear regression

-   Random Forest (bagging)

-   Gradient boosted decision trees (gradient boosting)

For the initial setup a set of default parameters are taken that are known to be a good starting point for various data sets.

```{r Specify models}
lmModel <-
  linear_reg(mode = "regression") %>%
  set_engine("lm")

rfModel <-
  rand_forest(mode = "regression",
              mtry = 3,
              trees = 500, 
              min_n = 5)%>% 
 set_engine("ranger")

xgModel <-
  boost_tree(mode = "regression",
            mtry = 3,
            trees = 500,
            min_n = NULL,
            tree_depth = 8,
            learn_rate = 0.3,
            loss_reduction = NULL,
            sample_size = NULL,
            stop_iter = 20) %>%
  set_engine("xgboost")

```

Each of the defined models is now fit to the training data set.

```{r Fit models}
lmFit <- fit(lmModel, int_rate ~ ., data = trainingSet_processed)
 rfFit <- fit(rfModel, int_rate ~ ., data = trainingSet_processed)
saveRDS(rfFit, "../Data/Models/rfFit.rds")
rfFit <- readRDS("../Data/Models/rfFit.rds")
xgFit <- fit(xgModel, int_rate ~ ., data = trainingSet_processed)
saveRDS(xgFit, "../Data/Models/xgFit.rds")
xgFit <- readRDS("../Data/Models/xgFit.rds")
```

Let's look at the output:

```{r linear regression output, include=FALSE}
lmFit %>% extract_fit_engine() %>%  summary()
tidy(lmFit)
```

```{r Random forest output, include=FALSE}
rfFit
```

```{r boosted Tree output, include=FALSE}
xgFit
data_mod <- data.frame(Iteration = xgFit$fit$evaluation_log$iter,  
                       RMSE = xgFit$fit$evaluation_log$training_rmse)
ggplot(data_mod, aes(x=Iteration, y=RMSE)) + geom_line()
```

## Define evaluation metrics

Next, we need to use these model fit objects to predict classes for the test set. After that, we will calculate and interpret the metrics describing the performance of the classifiers we trained. We need to start by creating a data frame including just the class outcome and the prediction.

```{r Create a data frame binding predictions to test set}
set.seed(713)

lmClassWithPredictions <- testSet_processed %>%
  dplyr::select(int_rate) %>%
  bind_cols(predict(lmFit, testSet_processed))

rfClassWithPredictions <- testSet_processed %>%
  dplyr::select(int_rate) %>%
  bind_cols(predict(rfFit, testSet_processed))

xgClassWithPredictions <- testSet_processed %>%
  dplyr::select(int_rate) %>%
  bind_cols(predict(xgFit, testSet_processed))

```

The `metric_set()` function from the "yardstick" package is used to define the metrics. For this project the following metrics have been chosen:

-   mean squared error (MSE)

    -   MSE is our main metric as MSE is sensitive to outliers. In regression tasks, it is important to consider the impact of outliers on the model performance. MSE gives more weight to larger errors, which makes          it more sensitive to outliers and can help identify issues with the model.

-   root mean squared error (RMSE)

    -   RMSE is our main metric as it puts a higher penalty on large errors. As we want to adjust our staffing according to the predicted calls a large offset would mean that we are either vastly over- or under             staffed.

-   root squared error (RSQ)

    -   RSQ is used as a second reference. It's purpose is to tell in relative terms how much the predictor variables account for the output.

-   mean absolute error (MAE)

    -   MAE is used as a third validation step as it is the the metric which is most easily interpreted

```{r Define metrics of interest}
metricSet <- metric_set(rmse, rsq, mae)
```

The metrics are now extracted and shown. Further there is a scatter plot to visualize the fit of the different models.

```{r Call the defined metric set as a function, echo=FALSE}
mLM <- metricSet(lmClassWithPredictions, truth = int_rate, estimate = .pred, event_level = "second")
mRF <- metricSet(rfClassWithPredictions, truth = int_rate, estimate = .pred, event_level = "second")
mXG <- metricSet(xgClassWithPredictions, truth = int_rate, estimate = .pred, event_level = "second")
  library(ggplot2) 
  library(gridExtra)
  library(grid)
  data_mod1 <- data.frame(Predicted = lmClassWithPredictions$.pred,  # Create data for ggplot2
                       Observed = lmClassWithPredictions$int_rate)
 p1 <-  ggplot(data_mod1,                                     
       aes(y = Predicted,
           x = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red")
  
    data_mod2 <- data.frame(Predicted = rfClassWithPredictions$.pred,  # Create data for ggplot2
                       Observed = rfClassWithPredictions$int_rate)
 p2<-  ggplot(data_mod2,                                     # Draw plot
       aes(y = Predicted,
           x = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red")
  
    data_mod3 <- data.frame(Predicted = xgClassWithPredictions$.pred,  # Create data for ggplot2
                       Observed = xgClassWithPredictions$int_rate)
  p3 <-ggplot(data_mod3,                                     # Draw plot
       aes(y = Predicted,
           x = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red")

grid.arrange(p1,p2,p3, ncol = 3, top=textGrob("Comparison between linear regression, random forest and Gradient boosted trees"))
```

We can see that the gradient boosted trees with xgBoost seem to perform best. XGBoost has the best RMSE `r mXG$.estimate[1]` as well as the best fitting plot. Is is notable that all models overestimate the number of callers in times where a low number of callers was observed.

In order to be able to also perform a hyperparamter tuning it was decided by the project to only move forward with the xgBoost model. It might be worth in the future to revisit also the linear regression and the random forest to see how well they can perform on the data given a more in depth configuration.

## Prepare the xgBoost model for hyperparameter tuning

Ah new model for the xgBoost is defined where all parameters are replaced with the 'tune()' function. This allows to have them defined by the tuning grid introduced in a later step.

```{r Define workflow, echo=FALSE}
xgbFit <- boost_tree(
  trees = tune(), 
  tree_depth = tune(),
  min_n = tune(), 
  loss_reduction = tune(),                 
  sample_size = tune(),
  mtry = tune(),         
  learn_rate = tune(),                   
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

xgbFit
```

For the tuning grid a space-filling design is used to cover the hyperparameter space as well as possible. "Experimental designs for computer experiments are used to construct parameter grids that try to cover the parameter space such that any portion of the space has an observed combination that is not too far from it." As size we used 30 but this should be increased to get better results. computation time did not allow for a bigger value here.

```{r Add fit, include=FALSE}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), trainingSet_processed), #treat mtry() differently because it depends
  #on the actual number of predictors in the data.
  learn_rate(),
  size = 50 #can be set to a higher value to improve results but is very time consuming
)

xgb_grid
```

Setup the workflow for later use.

```{r setup workflow}
xgb_wf <- workflow() %>%
  add_recipe(preprocRecipe)  %>%
  add_model(xgbFit)
xgb_wf
```

For training we setup a 5-fold cross validation. Here a 5 fold was select to get a good return of invested time. Increasing the number of folds would improve the stability of the model but would also increase significantly. If a high bias is observed when evaluating the model the number of folds should be increased. We also apply again stratification on the class variable, due to the right skewness of the data.

```{r setup vfold}
set.seed(123)
vb_folds <- vfold_cv(trainingSet, v = 5, strata = int_rate) 
```

Using the defined tuning grid, the cross validation and our defined metrics we can now search for the best hyperparameters.

```{r tune grid}
# all_cores <- parallel::detectCores(logical = FALSE)
# 
# library(doParallel)
# cl <- makePSOCKcluster(all_cores)
# registerDoParallel(cl)
# 
# set.seed(123)
# xgb_res <- tune_grid(
#   xgb_wf,
#   resamples = vb_folds,
#   grid = xgb_grid,
#   metrics = metricSet,
#   control = control_grid(save_pred = TRUE)
# )
# stopCluster(cl)
# # Save a single object to a file
# saveRDS(xgb_res, "../Data/Models/xgb_res.rds")
xgb_res <- readRDS("../Data/Models/xgb_res.rds")
m <- collect_metrics(xgb_res)
```

Below we see a visualization of the tuning grid runs. On the Y-Axis the RMSE is shown and the X-Axis shows the tuned hyperparamter values. - Increased learn rate reduces the RMSE - in the rest of the attributes no clear correlation is visible. There are a couple of noticeable data points such the min_n at 15 and 35. With increased size of the tuning grid this could get more visible.

```{r results}
xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "rmse")
```

As we defined the RMSE as our main metric we will take the model with the best RMSE value and save it.

```{r best rmse, include=FALSE}
knitr::kable(show_best(xgb_res, "rmse"))
best_rmse <- select_best(xgb_res, "rmse")
```

Having found the best set of hyper parameters we can finalize the workflow. In the output below the final model paramters as well as the influence of each hyperparmeter is shown. We see that by far the most important influencer is 17:00 as a time of day.

```{r finalize workflow }
final_xgb <- finalize_workflow(
  xgb_wf,
  best_rmse
)

final_xgb
```

The final model is now fit with the training and testdata set. Below we cn see the individual metrics and the the Plot that shows the predicted class values vs. the real data. We can see that with an RMSE of 'r collect_metrics(final_res)' and the plot that shows very little variance we are in a good spot with our model. The only negative thing that stands out is

```{r final fit}
final_res <- last_fit(final_xgb, trainIndex, metrics=metricSet)
saveRDS(final_res, "../Data/Models/final_res.rds")
final_res <- readRDS("../Data/Models/final_res.rds")
```

```{r final fit plot, echo=FALSE, warning=FALSE}
v1 <-extract_fit_parsnip(final_res) %>%
vip(geom = "point", aesthetics = list(color = "blue", size = 1.5, top="Test"))

fit <- extract_fit_parsnip(final_res)
data_mod <- data.frame(Iteration = fit$fit$evaluation_log$iter,  
                       RMSE = fit$fit$evaluation_log$training_rmse)
v2 <- ggplot(data_mod, aes(x=Iteration, y=RMSE)) + geom_line() + ggtitle("RMSE improvement over iterations")

v1 + ggtitle("Variable Importance")
v2

knitr::kable(collect_metrics(final_res))

```

\pagebreak
## Interpreting the results

In a final step we analyze our model and try to interpret it.

```{r prepare plot data, warning=FALSE, include=FALSE}
final_model <- final_res$.workflow[[1]]

# enrich predictions with train / test classifier and the absolute error AE to produce separated graphs
all_predictions <- bind_rows(
  augment(final_model, new_data = trainingSet) %>% 
    mutate(type = "train") %>% 
  mutate(AE = abs(.pred-int_rate)),
  augment(final_model, new_data = testSet) %>% 
    mutate(type = "test")  %>% 
    mutate(AE = abs(.pred-int_rate))
)
```

First we check how the training data compares to the test data. We see that the model performs slightly better on the training data, but does not have a large bias against the data. This can be seen both in the graph below, where the results look very similar, and in the individual metrics. We however again see that we are not good at predicting if the actual number of calls is low. This can be seen in the lower part of the plot were we have rather large residuals.

```{r plot train vs test, echo=FALSE, warning=FALSE}
all_predictions %>%
  ggplot(aes(int_rate, .pred)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 1) +
  facet_wrap(~type)   +
  ggtitle("Compare fit on test and training set")
```

We can see that this error is very pronounced on Mondays, Tuesdays and Fridays. Further we can see that on weekends we can expect less calls in general.

```{r plot plot fit per weekday, echo=FALSE, warning=FALSE}
all_predictions %>%
  dplyr::filter(type=="test")%>% 
  ggplot(aes(int_rate, .pred)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 1) +
  facet_wrap(~term) +
  ggtitle("Compare fit per term")

```

We want to analyse the weak predictions when the number of calls is low in a little more detail. We know that we have fewer calls at night. This shows also in graph below, which shows that the absolute error is higher at night when the number of calls is low.

```{r plot absolut error per hour, echo=FALSE, warning=FALSE}
all_predictions %>%
  dplyr::filter(type=="test")%>% 
  ggplot(aes(term, AE)) +
  geom_point()  +
  ggtitle("Aboslute error per term")

```

We can observe this pattern also on the individual days of the week.

```{r plot absolut error per hour by weekday, echo=FALSE, warning=FALSE}
all_predictions %>%
  ggplot(aes(term, AE)) +
  geom_point() +
  facet_wrap(~term) +
  ggtitle("Aboslute error per hour of the day for each day of the week")
```

```{r include=FALSE}
t1 <- all_predictions %>%
  filter( type =="train")%>%
  group_by(type) %>%
  metrics(int_rate, .pred)

t2 <- all_predictions %>%
  filter( type =="test")%>%
  group_by(type) %>%
  metrics(int_rate, .pred)
```

If we now compare the metrics for the model during day time hours (07:00 -19:00), RMSE: `r t1$.estimate[1]` with the night time hours (19:00 -07:00,RMSE: `r t2$.estimate[1]` ) we see that the model is more than 6 times more accurate during the day. We can conclude that our model performs very good during the day but has potential for improvement during the night.

```{r summary of predictions, echo=FALSE, warning=FALSE}

knitr::kable(list(caption = "train",t1, "test", t2) ,caption = "Metrics comparison" )

```

FOr the assigment we also need to calculate the MSE. This metric is not offered by yardstick so we have to calcualte it ourselves:
```{r}
m1 <- all_predictions %>%
  filter( type =="test")%>%
  group_by(type)

m2 <- all_predictions %>%
  filter( type =="train")%>%
  group_by(type)


mse <- function(predictions, actual) {
  diff <- predictions - actual
  squared_diff <- diff^2
  mean_squared_error <- mean(squared_diff)
  return(mean_squared_error)
}

mse_test <- mse(m1$.pred,m1$int_rate)
mse_train <- mse(m2$.pred,m2$int_rate)




```



##Making predictions with the model

The model can now be used to make predictions. In the example below it can be seen that on the 3rd of April at 17:00 you can expect almost 4 times more calls per hour depending or not whether it is a holiday.

In a second example it is demonstrated on how the day of the week has a significant impact on the number of callers to expect. On a Friday in July you can expect roughly five times more calls in a given hour than on a Sunday.

```{r}
# df <- data.frame(
#   Weekday=c("Monday","Monday","Friday","Sunday"),
#   Month =c("4","4","7","7"),
#   Day = c("3","3","8","8"), 
#   Hour =c("17","17","12","12"),
#   IsHoliday = c("1","0","0","0"),
#   W_TOTAL_AVG_1h = c(15,15,15,15))
# 
# new_predictions <- bind_rows(
#   augment(final_model, new_data = df, interval = "prediction"))
# 
# knitr::kable(new_predictions, caption = "Prediciton results:")
```

## Verdict and next steps

The presented model can predict the number of incoming calls per hour in a reasonable level of accuracy during the day and produces worse but still acceptable results during the night. To further improve the model the following options are available:

-   Use more source data. There are currently only 1.5 years of data available.

-   Explore other types of models in more detail: Random Forest, Neural networks etc.

-   Spend more time on the tuning of the hyper parameters for the xgBoost model.

-   It would also be possible to use an enriched input data set. The data can be enriched with weather data for example to see if the temperature on that day has a meaningful impact.

-   Use a dedicated model for day and night to get better predictions.

Also to make the model more robust it would be preferable to not only use a test and training split but also a validation split. More data and increased number of folds in the cross-validation would also be preferable but was not performed for this model due to time constraints.

## Sources

The following tutorials where used as a reference for this report:

-   Ross, J. (n.d.). Tidymodels Tutorial. RPubs. Retrieved December 9, 2022, from <https://rpubs.com/jwross83/933687>
-   Barter, R. (2020, April 15). Tidymodels: Tidy Machine Learning in R. Hugo Future Imperfect. Retrieved December 9, 2022, from <https://www.rebeccabarter.com/blog/2020-03-25_machine_learning/>
-   RichardOnData. (n.d.). Youtube-scripts/R tutorial (ML) - tidymodels.rmd at master Â· RICHARDONDATA/youtube-scripts. GitHub. Retrieved December 9, 2022, from https://github.com/RichardOnData/YouTube-Scripts/blob/master/R%20Tutorial%20(ML)%20-%20tidymodels.Rmd 

